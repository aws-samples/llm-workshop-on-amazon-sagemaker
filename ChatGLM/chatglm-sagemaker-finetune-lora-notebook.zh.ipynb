{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f55389-9564-41e7-a8ae-0bbd0904ced3",
   "metadata": {},
   "source": [
    "# ChatGLM-6B-LoRA\n",
    "\n",
    "本示例介绍 ChatGLM-6B 模型基于 LoRA 的微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd34a44-77f7-464c-bd98-4147deac0c56",
   "metadata": {
    "tags": []
   },
   "source": [
    "下面以 [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) 数据集为例介绍代码的使用方法。\n",
    "\n",
    "SageMaker Notebook kernel 环境选择 **conda_pytorch_p310**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29429dfe-5a3f-4a95-83c6-fefd48730a57",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "### LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9d1b6-f065-45ea-9d6f-08efcce40e3c",
   "metadata": {},
   "source": [
    "下载代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72fe065b-3f15-4947-8ded-9252e5fc1342",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ChatGLM-Tuning'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD is now at 9973930 Merge pull request #142 from mymusise/fix/infer_with_int8\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "rm -rf ChatGLM-Tuning\n",
    "git clone https://github.com/mymusise/ChatGLM-Tuning.git\n",
    "cd ChatGLM-Tuning\n",
    "git reset --hard 997393046a49510e6cda36962f9a399297959311"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fc4424-600e-4ef8-b27d-45d4e7da5ae6",
   "metadata": {},
   "source": [
    "### 软件依赖\n",
    "运行微调需要4.27.1版本的`transformers`。除 ChatGLM-6B 的依赖之外，还需要安装以下依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea918081-45af-4b26-ba6e-a6737e040711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting git+https://github.com/huggingface/peft.git (from -r ChatGLM-Tuning/requirements.txt (line 15))\n",
      "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-pauqpzfk\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-pauqpzfk\n",
      "  Resolved https://github.com/huggingface/peft.git to commit 032fff92fb74b737a2934e91a08d82142fb79dc3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bitsandbytes==0.37.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 2)) (0.37.1)\n",
      "Requirement already satisfied: accelerate==0.17.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 3)) (0.17.1)\n",
      "Requirement already satisfied: protobuf<3.20.1,>=3.19.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 6)) (3.20.0)\n",
      "Requirement already satisfied: transformers==4.27.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 7)) (4.27.1)\n",
      "Requirement already satisfied: icetk in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 8)) (0.0.4)\n",
      "Requirement already satisfied: cpm_kernels==1.0.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 9)) (1.0.11)\n",
      "Requirement already satisfied: torch>=1.13.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 10)) (2.0.0)\n",
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 11)) (2.13.0)\n",
      "Requirement already satisfied: datasets==2.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from -r ChatGLM-Tuning/requirements.txt (line 14)) (2.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.17.1->-r ChatGLM-Tuning/requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.17.1->-r ChatGLM-Tuning/requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.17.1->-r ChatGLM-Tuning/requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate==0.17.1->-r ChatGLM-Tuning/requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-Tuning/requirements.txt (line 7)) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-Tuning/requirements.txt (line 7)) (0.15.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-Tuning/requirements.txt (line 7)) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-Tuning/requirements.txt (line 7)) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-Tuning/requirements.txt (line 7)) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.27.1->-r ChatGLM-Tuning/requirements.txt (line 7)) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (0.18.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from icetk->-r ChatGLM-Tuning/requirements.txt (line 8)) (0.15.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from icetk->-r ChatGLM-Tuning/requirements.txt (line 8)) (0.1.99)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.1->-r ChatGLM-Tuning/requirements.txt (line 10)) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.1->-r ChatGLM-Tuning/requirements.txt (line 10)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.1->-r ChatGLM-Tuning/requirements.txt (line 10)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch>=1.13.1->-r ChatGLM-Tuning/requirements.txt (line 10)) (3.1.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (1.56.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (2.21.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (3.4.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (67.7.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (2.3.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (0.40.0)\n",
      "Requirement already satisfied: safetensors in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from peft==0.4.0.dev0->-r ChatGLM-Tuning/requirements.txt (line 15)) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (1.26.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.17.1->-r ChatGLM-Tuning/requirements.txt (line 3)) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r ChatGLM-Tuning/requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.27.1->-r ChatGLM-Tuning/requirements.txt (line 7)) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->datasets==2.10.1->-r ChatGLM-Tuning/requirements.txt (line 14)) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch>=1.13.1->-r ChatGLM-Tuning/requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torchvision->icetk->-r ChatGLM-Tuning/requirements.txt (line 8)) (9.5.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r ChatGLM-Tuning/requirements.txt (line 11)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ChatGLM-Tuning/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c14c4-ee95-4b57-9441-7d7efaedd441",
   "metadata": {},
   "source": [
    "### 清洗数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74796e4-3425-4e3d-9205-d99bcc4ffe11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formatting..: 100%|███████████████████| 52002/52002 [00:00<00:00, 180192.34it/s]\n"
     ]
    }
   ],
   "source": [
    "!python ChatGLM-Tuning/cover_alpaca2jsonl.py \\\n",
    "    --data_path ChatGLM-Tuning/data/alpaca_data.json \\\n",
    "    --save_path ChatGLM-Tuning/data/alpaca_data.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78969515-ac8e-4fad-b8e8-500cdfae30b9",
   "metadata": {},
   "source": [
    "### 生成数据集\n",
    "1. 使用ChatGLM Tokenizer对数据进行编码\n",
    "2. 依据Alpaca数据集格式进行Instruction FineTune\n",
    "\n",
    "**根据数据集的实际情况调整下述max_seq_length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc1090ed-0f3c-4cb3-8e32-aa38e12dd3d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/home/ec2-user/.cache/huggingface/datasets/generator/default-35c9670637d69edf/0.0.0)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!python ChatGLM-Tuning/tokenize_dataset_rows.py \\\n",
    "    --jsonl_path ChatGLM-Tuning/data/alpaca_data.jsonl \\\n",
    "    --save_path ChatGLM-Tuning/data/alpaca \\\n",
    "    --max_seq_length 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed75a5e-f2da-4683-80f2-47cbdb3c4933",
   "metadata": {},
   "source": [
    "### 加载原始模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31cf330c-34f5-4fe0-ab83-0e241c02c3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/ec2-user/anaconda3/envs/pytorch_p310 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.9/site-packages/torch/lib'), PosixPath('/opt/amazon/efa/lib')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.8/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d30ad8be6f459b95f4b5e2af38a9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac897926-df1d-45e4-aa31-8a35e5cbf18f",
   "metadata": {},
   "source": [
    "### 原始模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071bcdbc-8e98-4773-bb4e-3785a89c812b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./ChatGLM-Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e62f7db7-3755-4d45-a1b2-dffa77a602de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1201: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Answer: 1. Eat a balanced diet that includes a variety of fruits, vegetables, whole grains, lean proteins, and healthy fats.\n",
      "2. Exercise regularly, either through physical activity or simply by walking or engaging in other low-impact activities.\n",
      "3. Get enough sleep each night, as sleep is essential for maintaining physical and mental health.\n",
      "### 1.Answer:\n",
      " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule. \n",
      "\n",
      "\n",
      "Instruction: What are the three primary colors?\n",
      "Answer: The three primary colors are red, blue, and yellow. These colors are used in painting, printing, and other visual arts to create colors that can be easily mixed together to create other colors.\n",
      "### 2.Answer:\n",
      " The three primary colors are red, blue, and yellow. \n",
      "\n",
      "\n",
      "Instruction: Describe the structure of an atom.\n",
      "Answer: The structure of an atom is made up of a nucleus, which is the central black hole of the atom, surrounded by a cloud of electrons. The electrons are negatively charged and orbit the nucleus in specific patterns, known as electrons' shells. The electrons in an atom are in constant motion, and their movement is governed by the laws of quantum mechanics. The size of an atom is very small, with the nucleus being about 10^-18 meters in diameter and the electrons being much smaller, typically ranging in size from 1 to 10^-9 meters. The electrons in an atom are in constant motion, and their movement is governed by the laws\n",
      "### 3.Answer:\n",
      " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
      "\n",
      "\n",
      "Instruction: How can we reduce air pollution?\n",
      "Answer: There are several ways to reduce air pollution:\n",
      "\n",
      "1. Reduce energy consumption: Reducing energy consumption can help reduce the amount of air pollution caused by burning fossil fuels. This can be achieved by using renewable energy sources, reducing energy efficiency, and transitioning to cleaner, more sustainable energy sources.\n",
      "2. Reduce plastic waste: Plastic pollution is a major contributor to air pollution. Reducing plastic waste can help reduce the amount of plastic that is released into the environment. This can be achieved by reducing the use of single-use plastics, recycling, and reducing the amount of plastic that is sent to landfills.\n",
      "3. Reduce car use: Using public transportation\n",
      "### 4.Answer:\n",
      " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
      "\n",
      "\n",
      "Instruction: Describe a time when you had to make a difficult decision.\n",
      "Answer: I had to make a difficult decision when I was faced with the option of continuing my studies or starting a new job. I had been working as a software engineer for several years and had achieved a good level of success in my field. However, I was feeling unfulfilled and wanted to pursue a different path.\n",
      "\n",
      "I decided to continue my studies and pursue a degree in a field that I was passionate about. I spent several months studying and preparing for my exams, but I also had to start a new job to make ends meet.\n",
      "\n",
      "The decision was difficult, but I knew that it was the right one for me. I continued\n",
      "### 5.Answer:\n",
      " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cover_alpaca2jsonl import format_example\n",
    "import json\n",
    "\n",
    "\n",
    "instructions = json.load(open(\"ChatGLM-Tuning/data/alpaca_data.json\"))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature[\"context\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = tokenizer.decode(out[0])\n",
    "        print(answer)\n",
    "        item['infer_answer'] = answer\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3564ad33-3adb-4da8-a1bb-776494976201",
   "metadata": {},
   "source": [
    "### 训练参数展示\n",
    "\n",
    "**1. LoRA参数选择不同, 训练参数占比也不同**\n",
    "\n",
    "**2. 加载LoRA模型后,只有每层LoRA矩阵权重需要更新、训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "154318c5-95b7-4c54-9a19-3d05e34629be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    \n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {trainable_model_params*100/all_model_params:.4f}%\"\n",
    "\n",
    "\n",
    "def print_trainable_non_trainable_layers(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Trainable: {name}\")\n",
    "        else:\n",
    "            print(f\"Not trainable: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa126b14-2797-4add-a918-b7483a8051a5",
   "metadata": {},
   "source": [
    "### 设置LoRA参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eec41f9-d9ce-4afa-9c82-971db55f9012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a85f428-979f-45bb-accd-fb61f14f87a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3670016\n",
      "all model parameters: 6176956416\n",
      "percentage of trainable model parameters: 0.0594%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(model))\n",
    "# print_trainable_non_trainable_layers(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f3f177-e663-43cc-b3e5-60e9f4fc8caa",
   "metadata": {},
   "source": [
    "### 提取训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67126f8-cf7c-40c2-ad88-651ca88f8d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset_path = \"ChatGLM-Tuning/data/alpaca/\"\n",
    "\n",
    "dataset = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "train_num = 500\n",
    "# train_num = 50\n",
    "\n",
    "mini_train_dataset = datasets.Dataset.from_dict(dataset[:train_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ac85a-cf6d-4871-a99a-d27daf391792",
   "metadata": {},
   "source": [
    "### 定义数据提取器和Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02ddafb0-8e77-4784-93f2-957fc6b57900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, HfArgumentParser\n",
    "\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "class ModifiedTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659803b-3fa3-42fe-b220-aefcb63ba4ac",
   "metadata": {},
   "source": [
    "### 定义训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "767dd8bb-0512-4005-944d-2a6ec9390ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"output\",\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    learning_rate = 1e-4,\n",
    "    max_steps=50,\n",
    "    save_steps=50,\n",
    "    # max_steps=1500,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    seed=0,\n",
    "    data_seed=0,\n",
    "    group_by_length=False,\n",
    ")\n",
    "\n",
    "trainer = ModifiedTrainer(\n",
    "    model=model,\n",
    "    train_dataset=mini_train_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae402158-c7cd-4deb-a93a-cfde213256f1",
   "metadata": {},
   "source": [
    "### 开启模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deab4b5f-44e2-422a-a08e-11350d152d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 01:28, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.625700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=2.6257489013671873, metrics={'train_runtime': 90.6931, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.551, 'total_flos': 127662990409728.0, 'train_loss': 2.6257489013671873, 'epoch': 0.1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "814558c8-55d8-4b42-bccb-089feffb049e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ChatGLMForConditionalGeneration(\n",
       "      (transformer): ChatGLMModel(\n",
       "        (word_embeddings): Embedding(130528, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x GLMBlock(\n",
       "            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SelfAttention(\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (query_key_value): Linear8bitLt(\n",
       "                in_features=4096, out_features=12288, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GLU(\n",
       "              (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
       "              (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): CastOutputToFloat(\n",
       "        (0): Linear(in_features=4096, out_features=130528, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51545743-0a8d-44df-8e4e-da3d70b877ed",
   "metadata": {},
   "source": [
    "### 使用微调后的模型进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b662efc0-e22b-4dbb-bb9b-3e0dec189f08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Answer: 1. Eat a balanced diet rich in fruits, vegetables, whole grains, lean proteins, and healthy fats.\n",
      "2. Exercise regularly to improve cardiovascular health and boost energy levels.\n",
      "3. Get enough sleep each night to help regulate your body's internal clock and improve overall health.\n",
      "### 1.Answer:\n",
      " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule. \n",
      "\n",
      "\n",
      "Instruction: What are the three primary colors?\n",
      "Answer: The three primary colors are red, blue, and yellow.\n",
      "### 2.Answer:\n",
      " The three primary colors are red, blue, and yellow. \n",
      "\n",
      "\n",
      "Instruction: Describe the structure of an atom.\n",
      "Answer: The structure of an atom is made up of a central nucleus, surrounded by electrons. The electrons are in constant motion around the nucleus, and they are negatively charged. The electrons are in constant motion, and they are negatively charged. The electrons are in constant motion, and they are negatively charged. The electrons are in constant motion, and they are negatively charged. The electrons are in constant motion, and they are negatively charged. The electrons are in constant motion, and they are negatively charged. The electrons are in constant motion, and they are negatively charged. The electrons are in constant motion, and they are negatively charged. The electrons are in constant motion, and they are negatively\n",
      "### 3.Answer:\n",
      " An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom. \n",
      "\n",
      "\n",
      "Instruction: How can we reduce air pollution?\n",
      "Answer: There are several ways to reduce air pollution:\n",
      "\n",
      "1. Reduce energy consumption and emissions from vehicles by improving traffic management and infrastructure, increasing public transportation, and reducing the use of personal vehicles.\n",
      "2. Reduce the use of fossil fuels by transitioning to clean energy sources and increasing the use of renewable energy sources.\n",
      "3. Reduce the use of single-use plastics by promoting recycling and reducing waste.\n",
      "4. Reduce the use of coal and other fossil fuels by promoting energy efficiency and reducing energy consumption.\n",
      "5. Reduce the use of industrial processes by promoting energy efficiency and reducing energy consumption.\n",
      "6. Reduce the use of chemicals in the\n",
      "### 4.Answer:\n",
      " There are a number of ways to reduce air pollution, such as shifting to renewable energy sources, encouraging the use of public transportation, prohibiting the burning of fossil fuels, implementing policies to reduce emissions from industrial sources, and implementing vehicle emissions standards. Additionally, individuals can do their part to reduce air pollution by reducing car use, avoiding burning materials such as wood, and changing to energy efficient appliances. \n",
      "\n",
      "\n",
      "Instruction: Describe a time when you had to make a difficult decision.\n",
      "Answer: I had to make a difficult decision when I decided to pursue a career in the field of computer science or to pursue a career in the field of music. I had been interested in both for a while, but I had to make a decision that would affect my future. I had to weigh the pros and cons of each and decide which path I wanted to take. It was a difficult decision, but I ultimately decided to pursue a career in computer science and it has been a great decision.\n",
      "### 5.Answer:\n",
      " I had to make a difficult decision when I was working as a project manager at a construction company. I was in charge of a project that needed to be completed by a certain date in order to meet the client’s expectations. However, due to unexpected delays, we were not able to meet the deadline and so I had to make a difficult decision. I decided to extend the deadline, but I had to stretch the team’s resources even further and increase the budget. Although it was a risky decision, I ultimately decided to go ahead with it to ensure that the project was completed on time and that the client’s expectations were met. The project was eventually successfully completed and this was seen as a testament to my leadership and decision-making abilities. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cover_alpaca2jsonl import format_example\n",
    "import json\n",
    "\n",
    "\n",
    "instructions = json.load(open(\"ChatGLM-Tuning/data/alpaca_data.json\"))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature[\"context\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = tokenizer.decode(out[0])\n",
    "        print(answer)\n",
    "        item['infer_answer'] = answer\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3284fb8-306c-4e01-948a-a957f6e5b372",
   "metadata": {},
   "source": [
    "### 保存模型和参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91880940-1025-47e7-8c14-3ef6548b5256",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def save_tunable_parameters(model, path):\n",
    "    saved_params = {\n",
    "        k: v.to(\"cpu\")\n",
    "        for k, v in model.named_parameters()\n",
    "        if v.requires_grad\n",
    "    }\n",
    "    torch.save(saved_params, path)\n",
    "\n",
    "model.save_pretrained(\"output\")\n",
    "# save_tunable_parameters(model, os.path.join(\"output\", \"adapter_model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c95454-f016-4ae4-8100-333d5555c24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bfc9252-72dc-443d-b88f-a44215d84c19",
   "metadata": {},
   "source": [
    "## 加载微调模型并进行推理\n",
    "由于笔记本实例显存限制, 建议“restart kernel”后再执行下述单元, 否则会出现加载模型错误的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41da7a0-1314-4e50-af97-ee45cc29541c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./ChatGLM-Tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea156bd9-9163-4882-8e51-0c2aee434160",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/ec2-user/anaconda3/envs/pytorch_p310 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/opt/amazon/efa/lib'), PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.9/site-packages/torch/lib')}\n",
      "  warn(msg)\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda-11.8/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f20d63e6f44a088c268f99f46272c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import PeftModel\n",
    "\n",
    "# 加载原始模型\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "\n",
    "# 加载LoRA模型\n",
    "model = PeftModel.from_pretrained(model, \"output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdeace2e-61f4-408d-bd1b-1d2b865d7760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ChatGLMForConditionalGeneration(\n",
       "      (transformer): ChatGLMModel(\n",
       "        (word_embeddings): Embedding(130528, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x GLMBlock(\n",
       "            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SelfAttention(\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (query_key_value): Linear8bitLt(\n",
       "                in_features=4096, out_features=12288, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GLU(\n",
       "              (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
       "              (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28340674-12c5-46b3-820f-dd10ddbd1e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1374: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give three tips for staying healthy.\n",
      "Answer: 1. Eat a balanced diet rich in fruits, vegetables, whole grains, lean proteins, and healthy fats.\n",
      "2. Exercise regularly to maintain a healthy weight and improve cardiovascular health.\n",
      "3. Get enough sleep each night to help your body recover and function properly.\n",
      "### 1.Answer:\n",
      " 1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule. \n",
      "\n",
      "\n",
      "Instruction: What are the three primary colors?\n",
      "Answer: The three primary colors are red, blue, and yellow.\n",
      "### 2.Answer:\n",
      " The three primary colors are red, blue, and yellow. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cover_alpaca2jsonl import format_example\n",
    "import json\n",
    "\n",
    "\n",
    "instructions = json.load(open(\"ChatGLM-Tuning/data/alpaca_data.json\"))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, item in enumerate(instructions[:5]):\n",
    "        feature = format_example(item)\n",
    "        input_text = feature[\"context\"]\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            temperature=0\n",
    "        )\n",
    "        answer = tokenizer.decode(out[0])\n",
    "        print(answer)\n",
    "        item['infer_answer'] = answer\n",
    "        print(f\"### {idx+1}.Answer:\\n\", item.get('output'), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b735ee-053a-4923-8327-97e937c2135a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
