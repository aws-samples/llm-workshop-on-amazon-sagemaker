{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37930da-e0a6-406c-8a6a-a062746c7077",
   "metadata": {},
   "source": [
    "# An sample to finetune LlaMa2-13B(Vicuna/FastChat) distruibutely on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb561e-2329-42bd-b280-119a8f06f6fe",
   "metadata": {},
   "source": [
    "**To avoid 'no left space' error, move the docker root directory**\n",
    "\n",
    "sudo systemctl stop docker\n",
    "\n",
    "sudo systemctl stop docker.socket \n",
    "\n",
    "sudo mv /var/lib/docker /home/ec2-user/SageMaker \n",
    "\n",
    "sudo ln -s /home/ec2-user/SageMaker/docker /var/lib/docker \n",
    "\n",
    "sudo systemctl start docker.socket\n",
    "\n",
    "sudo systemctl start docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eedcfaa-83cf-4645-a5d3-8eaa5a0dae30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.173.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.175.0.tar.gz (857 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.4/857.4 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.26.157)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.24.3)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.20.0)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.5.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.157 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.29.157)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.157->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.175.0-py2.py3-none-any.whl size=1165569 sha256=34f46016649397584b7b87c7d846aab7517a12bdc0609b1bb9780a1d27de1afb\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/58/54/0f/d5ae0c7138ed9199780b15cc06cfec3666c31988829c3e255a\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.173.0\n",
      "    Uninstalling sagemaker-2.173.0:\n",
      "      Successfully uninstalled sagemaker-2.173.0\n",
      "Successfully installed sagemaker-2.175.0\n"
     ]
    }
   ],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df8dbba-4382-4fe5-9e6d-b75e1ca72bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sess                     = sagemaker.Session()\n",
    "role                     = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account                  = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region                   = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f07aa5a-b488-4294-8262-83023fc75adf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'FastChat'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD is now at 974537e Fix Llama2 formatting (#1997)\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "rm -rf src\n",
    "mkdir src\n",
    "cp s5cmd src/\n",
    "cd src\n",
    "\n",
    "git clone https://github.com/lm-sys/FastChat.git\n",
    "cd FastChat\n",
    "git reset --hard 974537efbd82093b45e64d07904efe7728193a52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ee865-66c0-4897-843d-194b4cec10e3",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b62370-89f7-4d8f-b5ee-a83781c88326",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc4f4ec6-3033-4cb3-845a-1f2e1fc0a080",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: huggingface_hub in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.16.4)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (3.12.0)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2023.5.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ac9937-a3f8-49fc-94ad-faadfb7752c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f295d2411964e73bfc58e2ddeb19e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd83f8f664c4b308d2e165aa02615d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb791c34bb8422f8dbee79a90537569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f21f2320734ffe934da7a534744003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8641043fdade4143bd07e51ddb7a308e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d449cb98d414a52a9709f46d31b5e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)134a50bb/config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb2823615ad4c99a3bd1e3216bbfc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873c1c85a3e84cf28852bef69a238cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c76b8f645e4152909caa5185467fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)a50bb/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf93b3dad2a47208c97f381479d49a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73e810c0b154becb0ecb2cb9a41cd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"TheBloke/Llama-2-13B-fp16\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\", \"*.py\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    "    revision='b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c20ac2-b016-4edd-81eb-d0f865f9234c",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43e753bc-1f5d-4e5a-8994-b079e267fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/config.json\n",
      "./model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/\n"
     ]
    }
   ],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f0e690-fa83-4952-81cf-78aa3d173ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/generation_config.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/generation_config.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/config.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/config.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/pytorch_model.bin.index.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/pytorch_model.bin.index.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/special_tokens_map.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/special_tokens_map.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/tokenizer_config.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/tokenizer_config.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/tokenizer.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/tokenizer.json\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/tokenizer.model s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/tokenizer.model\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/pytorch_model-00003-of-00003.bin s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/pytorch_model-00003-of-00003.bin\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/pytorch_model-00002-of-00003.bin s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/pytorch_model-00002-of-00003.bin\n",
      "cp model/models--TheBloke--Llama-2-13B-fp16/snapshots/b2e65e8ad4bb35e5abaee0170ebd5fc2134a50bb/pytorch_model-00001-of-00003.bin s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/pytorch_model-00001-of-00003.bin\n"
     ]
    }
   ],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llm/models/llama2/TheBloke/Llama-2-13B-fp16/ \n",
    "\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76be42-cc9b-48dd-8d12-ab60d5c2d3e2",
   "metadata": {},
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23458d2c-eb6a-4e94-a56e-32392e9dd699",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN pip3 uninstall -y deepspeed \\\n",
    "    && pip3 install deepspeed==0.10.0 \\\n",
    "    && pip3 install transformers==4.30.2\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5d016e-2b6b-453d-a4a5-84062d2aed0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2e4ea-4ae9-473d-92d8-e11a56c1d499",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "075cd597-cf50-4c90-a2ac-110b72c9f202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-vicuna-llama2-13b-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa8c4c49-f234-4356-802c-616b79e3975e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  72.35MB\n",
      "Step 1/6 : From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      " ---> c5a6ef695006\n",
      "Step 2/6 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 69955d9dc48e\n",
      "Step 3/6 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> a90673523815\n",
      "Step 4/6 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 892cf0574ff8\n",
      "Step 5/6 : RUN pip3 uninstall -y deepspeed     && pip3 install deepspeed==0.10.0     && pip3 install transformers==4.30.2\n",
      " ---> Running in 44f2c13323a5\n",
      "Found existing installation: deepspeed 0.6.1+06f2048\n",
      "Uninstalling deepspeed-0.6.1+06f2048:\n",
      "  Successfully uninstalled deepspeed-0.6.1+06f2048\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mCollecting deepspeed==0.10.0\n",
      "  Downloading deepspeed-0.10.0.tar.gz (836 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 836.6/836.6 kB 15.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (3.1.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (1.11.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (23.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (5.9.4)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (9.0.0)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (1.10.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (1.13.1+cu117)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic<2.0.0->deepspeed==0.10.0) (4.4.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py): started\n",
      "  Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.10.0-py3-none-any.whl size=877456 sha256=76990b36a83dc6e2370ef2f4e06073e8395dfb2bd4c6612396b16f2604d3a3c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/b6/e6/3c/20de0100098473aa1d879122fdd4a8491135ea2ea8e134e955\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: deepspeed\n",
      "Successfully installed deepspeed-0.10.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0 -> 23.2.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting transformers==4.30.2\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 21.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.30.2) (2.28.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.30.2) (1.23.5)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 43.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers==4.30.2) (4.64.1)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 40.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers==4.30.2) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers==4.30.2) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.30.2) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.30.2) (3.9.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers==4.30.2) (0.13.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.30.2) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.30.2) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.30.2) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.30.2) (2.1.1)\n",
      "Installing collected packages: safetensors, huggingface-hub, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.0\n",
      "    Uninstalling transformers-4.26.0:\n",
      "      Successfully uninstalled transformers-4.26.0\n",
      "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 transformers-4.30.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0 -> 23.2.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 44f2c13323a5\n",
      " ---> 8c2c84cb0d86\n",
      "Step 6/6 : ENV NVIDIA_VISIBLE_DEVICES=\"all\"\n",
      " ---> Running in caaf8304f2af\n",
      "Removing intermediate container caaf8304f2af\n",
      " ---> 49bbb3171945\n",
      "Successfully built 49bbb3171945\n",
      "Successfully tagged sagemaker-vicuna-llama2-13b-finetune:latest\n",
      "The push refers to repository [928808346782.dkr.ecr.us-west-2.amazonaws.com/sagemaker-vicuna-llama2-13b-finetune]\n",
      "3c6d175150f7: Preparing\n",
      "f8dae5c3df1e: Preparing\n",
      "e3221f18601a: Preparing\n",
      "b6f286626882: Preparing\n",
      "76fe97d80cdb: Preparing\n",
      "f5f76489fff8: Preparing\n",
      "621c3f07daa7: Preparing\n",
      "9b484bb42e11: Preparing\n",
      "54c7c0b58471: Preparing\n",
      "c34adc3ab668: Preparing\n",
      "bbf651e48b84: Preparing\n",
      "f61045791108: Preparing\n",
      "4e2ac0cda74a: Preparing\n",
      "658a33d555eb: Preparing\n",
      "bd16d9a61a98: Preparing\n",
      "f0c0cd2accfa: Preparing\n",
      "1275469c066c: Preparing\n",
      "b802dd3babf4: Preparing\n",
      "a3834ec63558: Preparing\n",
      "63edcef6dedf: Preparing\n",
      "0154e84cc2dd: Preparing\n",
      "f5f76489fff8: Waiting\n",
      "7085d1c151f6: Preparing\n",
      "a77a2104cfb6: Preparing\n",
      "621c3f07daa7: Waiting\n",
      "6808e7f9da2f: Preparing\n",
      "9b484bb42e11: Waiting\n",
      "3bc059a9dec6: Preparing\n",
      "de783f3fec23: Preparing\n",
      "54c7c0b58471: Waiting\n",
      "18ca52d74b2f: Preparing\n",
      "73df6ccd636c: Preparing\n",
      "c34adc3ab668: Waiting\n",
      "6738b73ff7a8: Preparing\n",
      "2a8292d9bfcc: Preparing\n",
      "bbf651e48b84: Waiting\n",
      "5b75a5ef32a7: Preparing\n",
      "25a5f55a11f0: Preparing\n",
      "707f484816ae: Preparing\n",
      "0430aa1e47d4: Preparing\n",
      "65448e793131: Preparing\n",
      "15af6e2d42ba: Preparing\n",
      "b46caef92993: Preparing\n",
      "53ce33a12646: Preparing\n",
      "aad68760f4ce: Preparing\n",
      "323d67ab1719: Preparing\n",
      "e72743a0fdfe: Preparing\n",
      "3996353f5820: Preparing\n",
      "ea87e0b9c30f: Preparing\n",
      "af18356cdf10: Preparing\n",
      "f6e30dd4497e: Preparing\n",
      "99832d04a153: Preparing\n",
      "a5981ed7a378: Preparing\n",
      "250519a2f830: Preparing\n",
      "6cadbde53f94: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "f61045791108: Waiting\n",
      "4e2ac0cda74a: Waiting\n",
      "658a33d555eb: Waiting\n",
      "707f484816ae: Waiting\n",
      "0430aa1e47d4: Waiting\n",
      "65448e793131: Waiting\n",
      "6808e7f9da2f: Waiting\n",
      "bd16d9a61a98: Waiting\n",
      "15af6e2d42ba: Waiting\n",
      "f0c0cd2accfa: Waiting\n",
      "b46caef92993: Waiting\n",
      "3bc059a9dec6: Waiting\n",
      "53ce33a12646: Waiting\n",
      "de783f3fec23: Waiting\n",
      "18ca52d74b2f: Waiting\n",
      "1275469c066c: Waiting\n",
      "aad68760f4ce: Waiting\n",
      "73df6ccd636c: Waiting\n",
      "323d67ab1719: Waiting\n",
      "6738b73ff7a8: Waiting\n",
      "b802dd3babf4: Waiting\n",
      "2a8292d9bfcc: Waiting\n",
      "e72743a0fdfe: Waiting\n",
      "5b75a5ef32a7: Waiting\n",
      "3996353f5820: Waiting\n",
      "ea87e0b9c30f: Waiting\n",
      "a3834ec63558: Waiting\n",
      "25a5f55a11f0: Waiting\n",
      "af18356cdf10: Waiting\n",
      "a77a2104cfb6: Waiting\n",
      "0154e84cc2dd: Waiting\n",
      "7085d1c151f6: Waiting\n",
      "f6e30dd4497e: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "a5981ed7a378: Waiting\n",
      "99832d04a153: Waiting\n",
      "250519a2f830: Waiting\n",
      "6cadbde53f94: Waiting\n",
      "f8dae5c3df1e: Pushed\n",
      "f5f76489fff8: Pushed\n",
      "e3221f18601a: Pushed\n",
      "621c3f07daa7: Pushed\n",
      "9b484bb42e11: Pushed\n",
      "b6f286626882: Pushed\n",
      "c34adc3ab668: Pushed\n",
      "3c6d175150f7: Pushed\n",
      "f61045791108: Pushed\n",
      "bbf651e48b84: Pushed\n",
      "4e2ac0cda74a: Pushed\n",
      "bd16d9a61a98: Pushed\n",
      "1275469c066c: Pushed\n",
      "b802dd3babf4: Pushed\n",
      "a3834ec63558: Pushed\n",
      "63edcef6dedf: Pushed\n",
      "54c7c0b58471: Pushed\n",
      "76fe97d80cdb: Pushed\n",
      "a77a2104cfb6: Pushed\n",
      "7085d1c151f6: Pushed\n",
      "3bc059a9dec6: Pushed\n",
      "de783f3fec23: Pushed\n",
      "18ca52d74b2f: Pushed\n",
      "73df6ccd636c: Pushed\n",
      "6738b73ff7a8: Pushed\n",
      "2a8292d9bfcc: Pushed\n",
      "0154e84cc2dd: Pushed\n",
      "f0c0cd2accfa: Pushed\n",
      "707f484816ae: Pushed\n",
      "0430aa1e47d4: Pushed\n",
      "65448e793131: Pushed\n",
      "5b75a5ef32a7: Pushed\n",
      "15af6e2d42ba: Pushed\n",
      "53ce33a12646: Pushed\n",
      "aad68760f4ce: Pushed\n",
      "6808e7f9da2f: Pushed\n",
      "e72743a0fdfe: Pushed\n",
      "3996353f5820: Pushed\n",
      "658a33d555eb: Pushed\n",
      "b46caef92993: Pushed\n",
      "f6e30dd4497e: Pushed\n",
      "99832d04a153: Pushed\n",
      "a5981ed7a378: Pushed\n",
      "250519a2f830: Pushed\n",
      "6cadbde53f94: Pushed\n",
      "0002c93bdb37: Pushed\n",
      "ea87e0b9c30f: Pushed\n",
      "25a5f55a11f0: Pushed\n",
      "323d67ab1719: Pushed\n",
      "af18356cdf10: Pushed\n",
      "latest: digest: sha256:a1f2eb9f64c92c406ef542e5f7fd71947af5edcb9b28bf43f345930815a7eb20 size: 10833\n"
     ]
    }
   ],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0b8a7-0733-46b0-b413-7f5620cfc626",
   "metadata": {},
   "source": [
    "### Generate the deepspeed config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3075b6-67df-4960-8815-6d1f14e416ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds.json\n",
    "{\n",
    "  \"bf16\": {\n",
    "    \"enabled\": true\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": false,\n",
    "    \"auto_cast\": false,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": \"auto\",\n",
    "      \"betas\": \"auto\",\n",
    "      \"eps\": \"auto\",\n",
    "      \"weight_decay\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": \"auto\",\n",
    "      \"warmup_max_lr\": \"auto\",\n",
    "      \"warmup_num_steps\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": true,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"sub_group_size\": 1e9,\n",
    "    \"reduce_bucket_size\": \"auto\",\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "    \"stage3_param_persistence_threshold\": \"auto\",\n",
    "    \"stage3_max_live_parameters\": 1e9,\n",
    "    \"stage3_max_reuse_distance\": 1e9,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"steps_per_print\": 2000,\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb15e9-107b-46cf-9644-ad7bc4c3231d",
   "metadata": {},
   "source": [
    "**Generate training entrypoint script.**\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49aa83-316b-45aa-931e-f18dbe003766",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llm/models/llama/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7879c-074c-4537-ba40-ee8d41a6747c",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "We modified some parts of ```FastChat/fastchat/train/train.py```, such as how to save model.\n",
    "\n",
    "The version is 2023-07-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87890eef-2bfa-4f75-879b-aa32be2dfdf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv src/FastChat/fastchat/train/train.py src/FastChat/fastchat/train/train_bak.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35790a97-4216-4126-a4e5-49860b337525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/FastChat/fastchat/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/FastChat/fastchat/train/train.py\n",
    "# This code is based on tatsu-lab/stanford_alpaca. Below is the original copyright:\n",
    "#\n",
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "\n",
    "from fastchat.conversation import SeparatorStyle\n",
    "from fastchat.model.model_adapter import get_conversation_template\n",
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    flash_attn: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the training data.\"}\n",
    "    )\n",
    "    lazy_preprocess: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conversation_template(\"vicuna\")\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    # Mask targets. Only compute loss on the assistant outputs.\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        turns = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_TOKEN_ID\n",
    "        for i, turn in enumerate(turns):\n",
    "            if turn == \"\":\n",
    "                break\n",
    "            turn_len = len(tokenizer(turn).input_ids)\n",
    "\n",
    "            parts = turn.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            # \"-2\" is hardcoded for the LLaMA tokenizer to make the offset correct.\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            # Ignore the user instructions\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "            cur_len += turn_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if False:  # Inspect and check the correctness of masking\n",
    "            z = target.clone()\n",
    "            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "            rank0_print(tokenizer.decode(z))\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_TOKEN_ID\n",
    "                rank0_print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "        attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in raw_data]\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(\n",
    "            input_ids=self.input_ids[i],\n",
    "            labels=self.labels[i],\n",
    "            attention_mask=self.attention_mask[i],\n",
    "        )\n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.raw_data = raw_data\n",
    "        self.cached_data_dict = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        if i in self.cached_data_dict:\n",
    "            return self.cached_data_dict[i]\n",
    "\n",
    "        ret = preprocess([self.raw_data[i][\"conversations\"]], self.tokenizer)\n",
    "        ret = dict(\n",
    "            input_ids=ret[\"input_ids\"][0],\n",
    "            labels=ret[\"labels\"][0],\n",
    "            attention_mask=ret[\"attention_mask\"][0],\n",
    "        )\n",
    "        self.cached_data_dict[i] = ret\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "def make_supervised_data_module(\n",
    "    tokenizer: transformers.PreTrainedTokenizer, data_args\n",
    ") -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = (\n",
    "        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n",
    "    )\n",
    "    rank0_print(\"Loading data...\")\n",
    "    \n",
    "    rank0_print(os.getcwd())\n",
    "    raw_data = json.load(open(data_args.data_path, \"r\"))\n",
    "\n",
    "    # Split train/test\n",
    "    np.random.seed(0)\n",
    "    perm = np.random.permutation(len(raw_data))\n",
    "    split = int(len(perm) * 0.98)\n",
    "    train_indices = perm[:split]\n",
    "    eval_indices = perm[split:]\n",
    "    train_raw_data = [raw_data[i] for i in train_indices]\n",
    "    eval_raw_data = [raw_data[i] for i in eval_indices]\n",
    "    rank0_print(f\"#train {len(train_raw_data)}, #eval {len(eval_raw_data)}\")\n",
    "\n",
    "    train_dataset = dataset_cls(train_raw_data, tokenizer=tokenizer)\n",
    "    eval_dataset = dataset_cls(eval_raw_data, tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n",
    "\n",
    "\n",
    "def train():\n",
    "    global local_rank\n",
    "\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    local_rank = training_args.local_rank\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "    # tokenizer.pad_token = tokenizer.unk_token\n",
    "    rank0_print(len(tokenizer))\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"-----------no pad token and add special token PAD----\")\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        rank0_print(len(tokenizer))\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n",
    "    trainer = Trainer(\n",
    "        model=model, tokenizer=tokenizer, args=training_args, **data_module\n",
    "    )\n",
    "\n",
    "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    # trainer.save_state()\n",
    "    # safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)\n",
    "    \n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86395940-1bda-4cb8-9465-60bf38bef57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/ds-train-dist.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/ds-train-dist.sh\n",
    "#!/bin/bash\n",
    "CURRENT_HOST=\"${SM_CURRENT_HOST}\"\n",
    "\n",
    "\n",
    "IFS=',' read -ra hosts_array <<< \"${SM_HOSTS}\"\n",
    "NNODES=${#hosts_array[@]}\n",
    "NODE_RANK=0\n",
    "\n",
    "for i in \"${!hosts_array[@]}\"; do\n",
    "    if [[ \"${hosts_array[$i]}\" == *${CURRENT_HOST}* ]]; then\n",
    "        echo \"host index：$i\"\n",
    "        NODE_RANK=\"$i\" \n",
    "    fi\n",
    "done\n",
    "   \n",
    "    \n",
    "MASTER_PORT=\"13579\"\n",
    "export NCCL_SOCKET_IFNAME=\"eth0\"\n",
    "\n",
    "#Configure the distributed arguments for torch.distributed.launch.\n",
    "GPUS_PER_NODE=\"$SM_NUM_GPUS\"\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE \\\n",
    "                  --nnodes $NNODES \\\n",
    "                  --node_rank $NODE_RANK \\\n",
    "                  --master_addr $MASTER_ADDR \\\n",
    "                  --master_port $MASTER_PORT\"\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llm/models/llama2/TheBloke/Llama-2-13B-fp16/* /tmp/llama_pretrain/\n",
    "\n",
    "cd FastChat && pip install -e . && cd ..\n",
    "\n",
    "\n",
    "DEEPSPEED_OPTS=\"\"\"\n",
    "    FastChat/fastchat/train/train_mem.py \n",
    "    --deepspeed ds.json \n",
    "    --model_name_or_path \"/tmp/llama_pretrain/\" \n",
    "    --data_path FastChat/data/dummy_conversation.json \n",
    "    --output_dir \"/tmp/llama_out\" \n",
    "    --num_train_epochs 1 \n",
    "    --per_device_train_batch_size 1 \n",
    "    --per_device_eval_batch_size  1 \n",
    "    --gradient_accumulation_steps 4 \n",
    "    --evaluation_strategy \"no\" \n",
    "    --save_strategy \"no\" \n",
    "    --save_steps 2000 \n",
    "    --save_total_limit 1 \n",
    "    --learning_rate 2e-5 \n",
    "    --weight_decay 0. \n",
    "    --warmup_ratio 0.03 \n",
    "    --lr_scheduler_type \"cosine\" \n",
    "    --logging_steps 1 \n",
    "    --cache_dir '/tmp' \n",
    "    --model_max_length 2048 \n",
    "    --gradient_checkpointing True \n",
    "    --lazy_preprocess True \n",
    "    --bf16 True \n",
    "    --tf32 True \n",
    "    --report_to \"none\"\n",
    "\"\"\"    \n",
    "\n",
    "CMD=\"torchrun ${DISTRIBUTED_ARGS} ${DEEPSPEED_OPTS}\"\n",
    "echo ${CMD}\n",
    "${CMD} 2>&1 \n",
    "\n",
    "if [[ \"${CURRENT_HOST}\" == \"${MASTER_ADDR}\" ]]; then  \n",
    "    ./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74742cbf-3de0-4221-b09d-24559370c65d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'928808346782.dkr.ecr.us-west-2.amazonaws.com/sagemaker-vicuna-llama2-13b-finetune:latest'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04ef0432-8dd5-4113-997b-50211448f654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "environment = {\n",
    "    'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'vicuna-llama2-finetune'\n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "# instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='ds-train-dist.sh',\n",
    "                      source_dir='./src',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bf5f8bd-fedc-4bba-9874-909195b762e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: llama-finetune-2023-08-05-08-35-20-831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-05 08:35:25 Starting - Starting the training job......\n",
      "2023-08-05 08:36:02 Starting - Preparing the instances for training........................\n",
      "2023-08-05 08:40:24 Downloading - Downloading input data\n",
      "2023-08-05 08:40:24 Training - Downloading the training image..................\n",
      "2023-08-05 08:43:00 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-05 08:43:59,905 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-05 08:43:59,961 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-05 08:43:59,971 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-05 08:43:59,972 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-05 08:44:00,964 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-05 08:44:01,032 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-05 08:44:01,099 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-05 08:44:01,109 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"llama-finetune-2023-08-05-08-35-20-831\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-928808346782/llama-finetune-2023-08-05-08-35-20-831/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds-train-dist.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds-train-dist.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds-train-dist.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds-train-dist.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-928808346782/llama-finetune-2023-08-05-08-35-20-831/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"llama-finetune-2023-08-05-08-35-20-831\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-928808346782/llama-finetune-2023-08-05-08-35-20-831/source/sourcedir.tar.gz\",\"module_name\":\"ds-train-dist.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds-train-dist.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./ds-train-dist.sh \"\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:44:02,424] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:44:05.157: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.2 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-08-05 08:44:05,161 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-08-05 08:44:05,183 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mhost index：0\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/tokenizer.json /tmp/llama_pretrain/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/pytorch_model-00003-of-00003.bin /tmp/llama_pretrain/pytorch_model-00003-of-00003.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/pytorch_model-00002-of-00003.bin /tmp/llama_pretrain/pytorch_model-00002-of-00003.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/llama2/TheBloke/Llama-2-13B-fp16/pytorch_model-00001-of-00003.bin /tmp/llama_pretrain/pytorch_model-00001-of-00003.bin\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code/FastChat\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting transformers<4.29.0,>=4.28.0\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 65.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting shortuuid\u001b[0m\n",
      "\u001b[34mDownloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (12.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.16.0)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 21.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<=2.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.1.97)\u001b[0m\n",
      "\u001b[34mCollecting markdown2[all]\u001b[0m\n",
      "\u001b[34mDownloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.101.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.7/65.7 kB 23.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (3.0.36)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.23.5)\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.24.1-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.4/75.4 kB 27.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tiktoken\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 108.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wandb\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 116.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.13.2)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.39.0-py3-none-any.whl (19.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.9/19.9 MB 64.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nh3\u001b[0m\n",
      "\u001b[34mDownloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 109.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.19) (0.2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic<=2.0->fschat==0.2.19) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.19) (2.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.19) (0.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (0.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate->fschat==0.2.19) (5.9.4)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.28.0,>=0.27.0\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 23.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting altair<6.0,>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.0.1-py3-none-any.whl (471 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.5/471.5 kB 77.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting websockets<12.0,>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 41.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.6/138.6 kB 40.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 15.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting gradio-client>=0.3.0\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.3.0-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.2/294.2 kB 59.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.1.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.5/50.5 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (9.4.0)\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.5/87.5 kB 25.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp~=3.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from uvicorn->fschat==0.2.19) (8.1.2)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 16.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.18.0,>=0.15.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.17.3-py3-none-any.whl (74 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.5/74.5 kB 24.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting wavedrom\u001b[0m\n",
      "\u001b[34mDownloading wavedrom-2.0.3.post3.tar.gz (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.7/137.7 kB 41.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.9/site-packages (from peft->fschat==0.2.19) (0.3.1)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.6/215.6 kB 55.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.5/188.5 kB 50.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting setproctitle\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->fschat==0.2.19) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->fschat==0.2.19) (1.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->fschat==0.2.19) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->fschat==0.2.19) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->fschat==0.2.19) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp~=3.0->gradio->fschat==0.2.19) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb->fschat==0.2.19) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 21.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from gradio-client>=0.3.0->gradio->fschat==0.2.19) (2023.1.0)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 29.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting mdurl~=0.1\u001b[0m\n",
      "\u001b[34mDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting linkify-it-py<3,>=1\u001b[0m\n",
      "\u001b[34mDownloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (1.0.7)\u001b[0m\n",
      "\u001b[34mCollecting mdit-py-plugins<=0.3.3\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.5/46.5 kB 12.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 15.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/41.0 kB 10.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.0/41.0 kB 13.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mDownloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting matplotlib~=3.0\u001b[0m\n",
      "\u001b[34mDownloading matplotlib-3.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 114.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of markupsafe to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting markupsafe~=2.0\u001b[0m\n",
      "\u001b[34mDownloading MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of markdown-it-py[linkify] to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting markdown-it-py[linkify]>=2.0.0\u001b[0m\n",
      "\u001b[34mDownloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.5/84.5 kB 30.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->fschat==0.2.19) (2022.7.1)\u001b[0m\n",
      "\u001b[34mCollecting svgwrite\u001b[0m\n",
      "\u001b[34mDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.1/67.1 kB 22.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.9/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->fschat==0.2.19) (1.1.0)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (0.19.3)\u001b[0m\n",
      "\u001b[34mCollecting uc-micro-py\u001b[0m\n",
      "\u001b[34mDownloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fschat, ffmpy, pathtools, wavedrom\u001b[0m\n",
      "\u001b[34mBuilding editable for fschat (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding editable for fschat (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fschat: filename=fschat-0.2.19-0.editable-py3-none-any.whl size=12630 sha256=481f2bb9db292d5d6d04ac43778335fdc177e08eba2b0111ecede33d4e670cd7\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-aij65ayf/wheels/db/bc/89/1dbc1ea5766ee06de619a402c8ee5502723e161b730e182b5b\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5580 sha256=a463f9438b9df38fe850c0ec9a7cda500b9ddeb6ef006dbaf4e6cfd8e5130be4\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/1f/f1/8d/367922b023b526b7c2ced5db30932def7b18cf39d7ac6e8572\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=92c0ecaa2bcef51d7e7f01f8181376115c4c1f78365d2205ec5b0e48f118ea60\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=29933 sha256=e287abbec4d6df7a525cdbaa71a748dcc3759be90abfc2f48eff6557fdeab277\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/81/08/ec/3e7bb60504c4ebf08e1d5c88e9abb85b0a3549d9f8d031113f\u001b[0m\n",
      "\u001b[34mSuccessfully built fschat ffmpy pathtools wavedrom\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, pathtools, nh3, ffmpy, websockets, uc-micro-py, typing-extensions, svgwrite, sniffio, smmap, shortuuid, setproctitle, sentry-sdk, semantic-version, python-multipart, orjson, mdurl, markdown2, h11, docker-pycreds, aiofiles, wavedrom, uvicorn, tiktoken, markdown-it-py, linkify-it-py, gitdb, anyio, transformers, starlette, mdit-py-plugins, httpcore, GitPython, altair, wandb, peft, httpx, fastapi, gradio-client, gradio, fschat\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.4.0\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.4.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.4.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.30.2\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.30.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.30.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.32 aiofiles-23.1.0 altair-5.0.1 anyio-3.7.1 docker-pycreds-0.4.0 fastapi-0.101.0 ffmpy-0.3.1 fschat-0.2.19 gitdb-4.0.10 gradio-3.39.0 gradio-client-0.3.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 linkify-it-py-2.0.2 markdown-it-py-2.2.0 markdown2-2.4.10 mdit-py-plugins-0.3.3 mdurl-0.1.2 nh3-0.2.14 orjson-3.9.2 pathtools-0.1.2 peft-0.4.0 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 sentry-sdk-1.29.2 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 sniffio-1.3.0 starlette-0.27.0 svgwrite-1.4.3 tiktoken-0.4.0 transformers-4.28.1 typing-extensions-4.7.1 uc-micro-py-1.0.2 uvicorn-0.23.2 wandb-0.15.8 wavedrom-2.0.3.post3 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mtorchrun --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_addr algo-1 --master_port 13579 FastChat/fastchat/train/train_mem.py --deepspeed ds.json --model_name_or_path /tmp/llama_pretrain/ --data_path FastChat/data/dummy_conversation.json --output_dir /tmp/llama_out --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy no --save_steps 2000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --cache_dir '/tmp' --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True --bf16 True --tf32 True --report_to none\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:06,414] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:06,414] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:06,414] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:06,414] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:06,414] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:06,416] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:06,416] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:06,422] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,314] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,314] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,314] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,314] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,315] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,315] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,315] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,315] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,315] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,315] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,333] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,333] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,365] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,365] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,365] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,365] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:10,365] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:45:13,873] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 13.02B parameters\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.63s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:09<00:19,  9.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 1/3 [00:16<00:32, 16.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:12, 12.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:12, 12.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:12, 12.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:12, 12.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:25<00:12, 12.96s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:25<00:12, 12.96s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:25<00:12, 12.98s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00,  9.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.20s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00,  9.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.23s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00,  9.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.23s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00,  9.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.24s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00,  9.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.24s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00,  9.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.24s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00,  9.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.25s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 2/3 [00:31<00:15, 15.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:41<00:00, 12.81s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 3/3 [00:41<00:00, 13.68s/it]\u001b[0m\n",
      "\u001b[34m32000\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m32001\u001b[0m\n",
      "\u001b[34mLoading data...\u001b[0m\n",
      "\u001b[34m/opt/ml/code\u001b[0m\n",
      "\u001b[34m#train 891, #eval 19\u001b[0m\n",
      "\u001b[34mFormatting inputs...Skip in lazy mode\u001b[0m\n",
      "\u001b[34mFormatting inputs...Skip in lazy mode\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.232191562652588 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.23123049736023 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.23116683959961 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.230908393859863 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.23117232322693 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.231220245361328 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.230690479278564 seconds\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.230852127075195 seconds\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 414720 in 81 params\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.003: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.003: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.003: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.004: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.004: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.004: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.008: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.030 algo-1:295 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.030 algo-1:294 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.030 algo-1:292 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.030 algo-1:296 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.030 algo-1:290 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.031 algo-1:291 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.035 algo-1:293 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.050 algo-1:292 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.050 algo-1:290 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.050 algo-1:295 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.050 algo-1:294 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.050 algo-1:291 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.050 algo-1:296 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.054 algo-1:293 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.120: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.147 algo-1:289 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-08-05 08:46:32.165 algo-1:289 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m{'loss': 3.1978, 'learning_rate': 0.0, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m{'loss': 1.7896, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m{'loss': 1.8164, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m{'loss': 0.562, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3114, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m{'loss': 0.2474, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m{'loss': 0.3002, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1782, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1621, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1545, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m{'loss': 0.128, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1323, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1351, 'learning_rate': 2e-05, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1462, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1506, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1331, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1566, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1315, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1292, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1231, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1422, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1568, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1287, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1167, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1246, 'learning_rate': 2e-05, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1154, 'learning_rate': 2e-05, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m{'loss': 0.1098, 'learning_rate': 2e-05, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m{'loss': 0.114, 'learning_rate': 2e-05, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'train_runtime': 182.7846, 'train_samples_per_second': 4.875, 'train_steps_per_second': 0.153, 'train_loss': 0.39618819100516184, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/added_tokens.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/config.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/generation_config.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/special_tokens_map.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/tokenizer_config.json s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/training_args.bin s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/tokenizer.model s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/tokenizer.model\u001b[0m\n",
      "\n",
      "2023-08-05 08:51:53 Uploading - Uploading generated training model\n",
      "2023-08-05 08:51:53 Completed - Training job completed\n",
      "\u001b[34mcp /tmp/llama_out/pytorch_model.bin s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2023-08-05 08:51:41,507 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-05 08:51:41,507 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-05 08:51:41,508 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 709\n",
      "Billable seconds: 709\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6f7d2-80b1-4332-81f5-65e8336910c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35aceef6-e0f4-4d12-8546-d0793db373e7",
   "metadata": {},
   "source": [
    "\n",
    "### Serve large models on SageMaker with DJL DeepSpeed Container\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the latest container launched using from DeepSpeed and DJL. DJL provides for the serving framework while DeepSpeed is the key sharding library we leverage to enable hosting of large models.We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/).\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. This notebook uses tensor parallelism techniques which allow GPUs to work simultaneously on the same layer of a model and achieve low latency inference relative to a pipeline parallel solution.\n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we deploy the open source llama 7B model across GPU's on a ml.g5.48xlarge instance. Note that the llama 7B fp16 model can be deployed on single GPU such as g5.2xlarge (24GB VRAM), we jsut show the code which can deploy the llm accross multiple GPUs in SageMaker. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers. For further reading on DeepSpeed you can refer to https://arxiv.org/pdf/2207.00032.pdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedb008-79dd-4c20-9509-53a374a2a478",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact and Upload Model to S3\n",
    "\n",
    "SageMaker needs the model to be in a Tarball format. In this notebook we are going to create the model with the Inference code to shorten the end point creation time. \n",
    "\n",
    "The tarball is in the following format\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "│   └── serving.properties\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- `model.py` is the key file which will handle any requests for serving. \n",
    "- `requirements.txt` has the required libraries needed to be installed when the container starts up.\n",
    "- `serving.properties` is the script that will have environment variables which can be used to customize model.py at run time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07349695-9935-4d1e-bebf-22a8d95623c0",
   "metadata": {},
   "source": [
    "#### Serving.properties has engine parameter which tells the DJL model server to use the DeepSpeed engine to load the model.\n",
    "\n",
    "option.tensor_parallel_degree:  if we use the g5.48xlarge which has 8 GPUs, so we set the tensor_parallel_degree to 8.\n",
    "\n",
    "option.s3url:  you should use your model path here. And the s3 path must be ended with \"/\".\n",
    "\n",
    "batch_size:   it is for server side batch based on request level. You can set batch_size to the large value which can not result in the OOM. The current code about model.py is just demo for one prompt per client request.\n",
    "\n",
    "max_batch_delay:   it is counted by millisecond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caf83dcf-c882-4d60-b75b-95141cafb804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf src\n",
    "!mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91feb354-4023-40ce-83a8-3c683c83a8e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/serving.properties\n",
    "engine=DeepSpeed\n",
    "option.tensor_parallel_degree=4\n",
    "option.s3url=s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-05-08-50-20/llama_out/\n",
    "batch_size=1\n",
    "max_batch_delay=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d10af817-98d3-41d1-8725-00246ca42668",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "transformers==4.30.2\n",
    "sagemaker\n",
    "nvgpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a727878-67c1-4e2f-8b5f-e059d10e54bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import deepspeed\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "\n",
    "\n",
    "predictor = None\n",
    "#here, we need to set the global variable batch_size according to the batch_size in the serving.properties file.\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_location, torch_dtype=torch.float16)\n",
    "\n",
    "    #for deepspeed inference \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_location, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "    print(\"----------model dtype is {0}---------\".format(model.dtype))\n",
    "    model = deepspeed.init_inference(\n",
    "        model,\n",
    "        mp_size=tensor_parallel,\n",
    "        dtype=torch.half,\n",
    "        replace_method=\"auto\",\n",
    "        replace_with_kernel_inject=True,\n",
    "    )\n",
    "        \n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "    generator = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, use_cache=True, device=local_rank)\n",
    "    \n",
    "    #for llama model, maybe the followiong code is need when you invoke the pipleline API for batch input prompts.\n",
    "    generator.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "    return generator, model, tokenizer\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global predictor, model, tokenizer\n",
    "    try:\n",
    "        if not predictor:\n",
    "            predictor,model,tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "        print(inputs)\n",
    "        if inputs.is_empty():\n",
    "            # Model server makes an empty call to warmup the model on startup\n",
    "            return None\n",
    "        \n",
    "        if inputs.is_batch():\n",
    "            #the demo code is just suitable for single sample per client request\n",
    "            bs = inputs.get_batch_size()\n",
    "            logging.info(f\"Dynamic batching size: {bs}.\")\n",
    "            batch = inputs.get_batches()\n",
    "            #print(batch)\n",
    "            tmp_inputs = []\n",
    "            for _, item in enumerate(batch):\n",
    "                tmp_item = item.get_as_json()\n",
    "                tmp_inputs.append(tmp_item.get(\"input\"))\n",
    "            \n",
    "            #For server side batch, we just use the custom generation parameters for single Sagemaker Endpoint.\n",
    "            result = predictor(tmp_inputs, batch_size = bs, max_new_tokens = 128, min_new_tokens = 128, temperature = 1.0, do_sample = True)\n",
    "            \n",
    "            outputs = Output()\n",
    "            for i in range(len(result)):\n",
    "                outputs.add(result[i], key=\"generate_text\", batch_index=i)\n",
    "            return outputs\n",
    "        else:\n",
    "            inputs = inputs.get_as_json()\n",
    "            if not inputs.get(\"input\"):\n",
    "                return Output().add_as_json({\"code\":-1,\"msg\":\"input field can't be null\"})\n",
    "\n",
    "            #input data\n",
    "            data = inputs.get(\"input\")\n",
    "            params = inputs.get(\"params\",{})\n",
    "            print(\"data  :{}\".format(data))\n",
    "            print(\"params:{}\".format(params))\n",
    "\n",
    "            #for pure client side batch\n",
    "            if type(data) == str:\n",
    "                bs = 1\n",
    "            elif type(data) == list:\n",
    "                if len(data) > batch_size:\n",
    "                    bs = batch_size\n",
    "                else:\n",
    "                    bs = len(data)\n",
    "            else:\n",
    "                return Output().add_as_json({\"code\":-1,\"msg\": \"input has wrong type\"})\n",
    "                \n",
    "            print(\"client side batch size is \", bs)\n",
    "            #predictor\n",
    "            result = predictor(data, batch_size = bs, **params)\n",
    "            print(\"result:{}\".format(data))\n",
    "            \n",
    "            #return\n",
    "            return Output().add({\"code\":0,\"msg\":\"ok\",\"data\":result})\n",
    "    except Exception as e:\n",
    "        return Output().add_as_json({\"code\":-1,\"msg\":e})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff846e6-2a3b-4305-a77e-d7797770297f",
   "metadata": {},
   "source": [
    "#### Create required variables and initialize them to create the endpoint, we leverage boto3 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6727e6d5-074f-4ac6-b553-b4a97f62056a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sage_session = sagemaker.Session()\n",
    "model_bucket = sage_session.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = (\n",
    "    \"llm/models/llama2/code-13b\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ed5da-49db-44c0-8f1f-2a416b6c4de9",
   "metadata": {},
   "source": [
    "**Image URI for the DJL container is being used here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bf1fdc1-1718-41d1-b06e-44075a51ceea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\n"
     ]
    }
   ],
   "source": [
    "#Note that: you can modify the image url according to your specific region.\n",
    "# inference_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    "\n",
    "inference_image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a87eb4-0b3b-4199-8438-55f160d19b64",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44652f16-8321-4b1a-a9bd-143eb80ff0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src/\n",
      "src/requirements.txt\n",
      "src/model.py\n",
      "src/serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm -f model.tar.gz\n",
    "!tar czvf model.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3bb8c4d-b4fd-444f-951c-25b7f01dc9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-928808346782/llm/models/llama2/code-13b/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_artifact = sage_session.upload_data(\"model.tar.gz\", model_bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "580bb4e8-ca46-49da-a1d3-59fbc930f250",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Model Bucket is -- > sagemaker-us-west-2-928808346782\n"
     ]
    }
   ],
   "source": [
    "print(f\"S3 Model Bucket is -- > {model_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b9914-3879-4839-a568-322d66bd0ea2",
   "metadata": {},
   "source": [
    "### To create the end point the steps are:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.g5.2xlarge \n",
    "    \n",
    "    b) ContainerStartupHealthCheckTimeoutInSeconds is 15*60 to ensure health check starts after the model is ready\n",
    "    \n",
    "3. Create the end point using the endpoint config created    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd73c8ca-835f-4300-9506-674376102c82",
   "metadata": {
    "tags": []
   },
   "source": [
    "One of the key parameters here is **TENSOR_PARALLEL_DEGREE** which essentially tells the DeepSpeed library to partition the models along 8 GPU's. This is a tunable and configurable parameter.\n",
    "\n",
    "This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests. For further reading on DeepSpeed you can follow the link https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08367121-837e-4dc0-9be5-97a3a83adeff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vicuna-13b-finetuned-2023-08-05-08-53-32-162\n",
      "Created Model: arn:aws:sagemaker:us-west-2:928808346782:model/vicuna-13b-finetuned-2023-08-05-08-53-32-162\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "model_name = name_from_base(f\"vicuna-llama2-13b-finetuned\")\n",
    "print(model_name)\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c503b35-787f-4e22-af96-cdfdc254f84e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:928808346782:endpoint-config/vicuna-13b-finetuned-2023-08-05-08-53-32-162-config',\n",
       " 'ResponseMetadata': {'RequestId': 'bd6ff807-c130-4c79-97f5-56ab57253307',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'bd6ff807-c130-4c79-97f5-56ab57253307',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '132',\n",
       "   'date': 'Sat, 05 Aug 2023 08:53:34 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.12xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            #\"VolumeSizeInGB\" : 300,\n",
    "            #\"ModelDataDownloadTimeoutInSeconds\": 15*60,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49e824c3-2472-4e1f-8853-061a55e1ade6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-west-2:928808346782:endpoint/vicuna-13b-finetuned-2023-08-05-08-53-32-162-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d72d7-8b49-4b21-be6c-83c9f29ecc8a",
   "metadata": {},
   "source": [
    "#### Wait for the end point to be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abc448-d60e-4b59-bfc6-65fde9858c25",
   "metadata": {},
   "source": [
    "### This step can take ~ 15 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9d330f0-4929-4cb2-93cf-2e595c23ca63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:928808346782:endpoint/vicuna-13b-finetuned-2023-08-05-08-53-32-162-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bfaff5-b4a0-4875-95b7-266320e2d30b",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text as a prompt and Model will complete the sentence and return the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb83cc51-172e-44b8-a00e-e26f669e957c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.8 ms, sys: 0 ns, total: 24.8 ms\n",
      "Wall time: 5.58 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"code\":0,\\n  \"msg\":\"ok\",\\n  \"data\":[\\n    {\\n      \"generated_text\":\"##Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That\\'s cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I\\'m more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That\\'s cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.##Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I\\'m more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That\\'s cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.##Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I\\'m more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That\\'s cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.#### Malcolm:Oh. What are you wearing right now, pet?## Eva:I am a language model called Vicuna, and I was trained by Large Model Systems Organization (LMSYS) researchers as a language model.## Vicuna: I\\'m Vicuna, a language model created by Large Model Systems Organization (LMSYS).## Vicuna: I\\'m developed by researchers from Large Model Systems Organization (LMSYS).## Vicuna: No, I am a language model developed by researchers from Large Model Systems Organization (LMSYS).\\\\nLarge Model Systems Organization (LMSYS) is a language model developed by researchers\"\\n    }\\n  ]\\n}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "prompt1 = \"The house is wonderful. I\"\n",
    "prompt2=\"##Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.##Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.##Eva: Yes, Sir. Queen is one of the most influential bands of all time.## Malcolm:It is. They are one of my favorite rock groups. What about you?## Eva:I'm more into classic rock, especially David Bowie. Who is your favorite artist?## Malcolm:Marylin Manson. You?## Eva:My favorite artist is David Bowie.## Eva:How often do you travel?## Malcolm:I like David Bowie too. I don’t travel much any more, but I used to.## Eva:That's cool! I recently took a road trip with my friend. We had so much fun and it opened up so many possibilities for us. What kind of places did you like to explore?## Malcolm:I love history and culture, so those are my favorite.## Eva: He was born in Birmingham, England and raised in Los Angeles, California.#### Malcolm:Oh. What are you wearing right now, pet?## Eva:\"\n",
    "\n",
    "parameters = {\n",
    "  \"early_stopping\": True,\n",
    "  \"max_new_tokens\": 128,\n",
    "  \"min_new_tokens\": 128,\n",
    "  \"do_sample\": True,\n",
    "  \"temperature\": 1.0,\n",
    "}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                #\"input\": prompt1,\n",
    "                \"input\": prompt2,\n",
    "                #\"input\": [prompt2,prompt2],\n",
    "                #\"input\": [prompt2,prompt2, prompt2,prompt2],\n",
    "                #\"input\": [prompt1,prompt1, prompt1,prompt1, prompt1,prompt1, prompt1,prompt1],\n",
    "                #\"input\": [prompt2,prompt2, prompt2,prompt2, prompt2,prompt2, prompt2,prompt2],\n",
    "                #\"input\": [prompt1, prompt2],\n",
    "                #\"input\": [prompt1, prompt2, prompt1, prompt2, prompt1, prompt2,prompt1, prompt2,],\n",
    "                \"params\": parameters\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "response_model['Body'].read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486f952-bbf5-4b7d-8fb3-4aea37df1a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
